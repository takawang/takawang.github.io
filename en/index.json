[{"content":"VSCode Extension Frontend HTML CSS Support (intelligence) CSS Peak (link to style) Prettier (format on save) Highlight Matching Tag Image preview Color highlight (Highlight web colors in your editor) Auto rename tag Live server ESLint Volar (Vue Language Features) Javascript Key Code Backend PHP Intelephense Django General TODO Highlight Chrome Extension Pesticide Vue.js devtools Live Server Web Extension Cheat Sheet CSS ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/extension-and-more/","summary":"Common VSCode Extension and More\u0026hellip;","title":"Some Extension \u0026 More"},{"content":" Behind the scenes when a web page is loaded and rendered:\nLoad HTML\nThe browser starts by fetching and loading the HTML file. Parse HTML\nThe HTML file is parsed to create the Document Object Model (DOM). The DOM represents the structure of the document as a tree of objects, where each object corresponds to a part of the document (e.g., elements, attributes). Load CSS\nIf there are linked or inline stylesheets in the HTML, the browser fetches and loads them. Parse CSS\nThe loaded CSS files are parsed to create the CSS Object Model (CSSOM). This represents the styles and layout information defined in the CSS. Resolve Conflicting CSS Declarations: If there are conflicting CSS declarations (i.e., rules that apply to the same element and property), the browser uses a specific set of rules to determine which styles take precedence. Process Final CSS Values: The browser processes the final computed values for each CSS property, taking into account inheritance, specificity, and other rules. Render Tree\nThe DOM and CSSOM are combined to create the Render Tree. The Render Tree represents the visual hierarchy of the elements on the page that will be rendered. It includes only the elements that will actually be displayed. The Visual Formatting Model\nThe visual formatting model takes the render tree and calculates the layout of each element on the page. This includes determining the size and position of each element based on its CSS properties. The visual formatting model also takes into account factors like the box model (margins, borders, padding), positioning, and floating elements. Render the Page\nFinally, the browser renders the page by painting each pixel according to the specifications of the visual formatting model. Image source: Critical CSS in WordPress: What It Is and How to Optimize CSS Delivery\n","permalink":"https://blog2.cmwang.net/en/posts/2023/11/css-crp/","summary":"CSS Critical Rendering Path","title":"Critical Rendering Path (CRP)"},{"content":"faster-whisper is a reimplementation of OpenAI\u0026rsquo;s Whisper model using CTranslate2, an engine designed for fast inference of Transformer models. The overall speed is significantly improved.\nBelow is a simple example of generating subtitles. First, install faster_whisper and pysubs2:\n# pip install faster_whisper pysubs2 from faster_whisper import WhisperModel import pysubs2 model = WhisperModel(model_size=\u0026#39;large-v2\u0026#39;) segments, _ = model.transcribe(audio=\u0026#39;audio.mp3\u0026#39;) # Prepare results for SRT file format results = [] for s in segments: segment_dict = {\u0026#39;start\u0026#39;: s.start, \u0026#39;end\u0026#39;: s.end, \u0026#39;text\u0026#39;: s.text} results.append(segment_dict) subs = pysubs2.load_from_whisper(results) subs.save(\u0026#39;output.srt\u0026#39;) # save srt file You can modify it to display a progress bar using tqdm:\nfrom faster_whisper import WhisperModel import pysubs2 from tqdm import tqdm model = WhisperModel(model_size=\u0026#39;large-v2\u0026#39;) segments, info = model.transcribe(audio=\u0026#39;audio.mp3\u0026#39;) # Prepare results for SRT file format results = [] timestamps = 0.0 # for progress bar with tqdm(total=info.duration, unit=\u0026#34; audio seconds\u0026#34;) as pbar: for seg in segments: segment_dict = {\u0026#39;start\u0026#39;: seg.start, \u0026#39;end\u0026#39;: seg.end, \u0026#39;text\u0026#39;: seg.text} results.append(segment_dict) # Update progress bar based on segment duration pbar.update(seg.end - timestamps) timestamps = seg.end # Handle silence at the end of the audio if timestamps \u0026lt; info.duration: pbar.update(info.duration - timestamps) subs = pysubs2.load_from_whisper(results) subs.save(\u0026#39;output.srt\u0026#39;) # save srt file Additionally, here\u0026rsquo;s a Dockerfile to set up the environment:\n# Use the official NVIDIA CUDA image as the base image FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04 ARG DEBIAN_FRONTEND=noninteractive # Install necessary dependencies RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ wget \\ python3 \\ python3-pip \\ \u0026amp;\u0026amp; apt-get clean \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # Set the working directory inside the container WORKDIR /app # Install required Python packages RUN pip install faster_whisper pysubs2 tqdm # Create directories to store the models RUN mkdir -p /models/faster-whisper-medium # Download the medium model using wget to the specified directory RUN wget -O /models/faster-whisper-medium/config.json https://huggingface.co/guillaumekln/faster-whisper-medium/resolve/main/config.json \u0026amp;\u0026amp; \\ wget -O /models/faster-whisper-medium/model.bin https://huggingface.co/guillaumekln/faster-whisper-medium/resolve/main/model.bin \u0026amp;\u0026amp; \\ wget -O /models/faster-whisper-medium/tokenizer.json https://huggingface.co/guillaumekln/faster-whisper-medium/resolve/main/tokenizer.json \u0026amp;\u0026amp; \\ wget -O /models/faster-whisper-medium/vocabulary.txt https://huggingface.co/guillaumekln/faster-whisper-medium/resolve/main/vocabulary.txt COPY app.py /app/ # Run the script CMD [\u0026#34;python3\u0026#34;, \u0026#34;app.py\u0026#34;] Source Code: https://github.com/taka-wang/docker-whisper\n","permalink":"https://blog2.cmwang.net/en/posts/2023/10/faster-whisper-in-python3/","summary":"How to use faster-whisper and generate a progress bar","title":"Faster Whisper With Progress Bar"},{"content":"Uninstall Docker sudo systemctl stop docker sudo apt remove --purge -y docker-ce docker-ce-cli containerd.io sudo rm -rf /var/lib/docker sudo rm -rf /etc/docker sudo rm -rf /etc/apt/sources.list.d/docker.list sudo rm -rf /usr/share/keyrings/docker-archive-keyring.gpg sudo apt update docker --version Install Docker (Ubuntu 22.04) sudo apt update sudo apt install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io # post install sudo systemctl status docker sudo usermod -aG docker ${USER} Move Docker Data to Another Location sudo systemctl stop docker sudo systemctl status docker # ex: /data/ is another disk, don\u0026#39;t create docker-data folder in advanced sudo rsync -avxP /var/lib/docker/ /data/docker-data # edit daemon.json with `data-root` line sudo nano /etc/docker/daemon.json { \u0026#34;data-root\u0026#34;: \u0026#34;/data/docker-data\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;args\u0026#34;: [], \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34; } } } sudo systemctl restart docker sudo systemctl enable docker # verify docker info -f \u0026#39;{{ .DockerRootDir}}\u0026#39; Install Nvidia Driver \u0026amp; Cuda wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2204-12-2-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda Install Nvidia Docker Toolkit curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\ \u0026amp;\u0026amp; \\ sudo apt-get update sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker # verify docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi Install Docker Compose # https://docs.docker.com/compose/install/ sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose References CUDA Toolkit 12.2 Update 2 Downloads ÂÆâË£ù Nvidia driver 535 CUDA 12.2 cudnn 12.x on Ubuntu 22.04 ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/install-nvidia-docker-for-ubuntu/","summary":"Uninstall Docker sudo systemctl stop docker sudo apt remove --purge -y docker-ce docker-ce-cli containerd.io sudo rm -rf /var/lib/docker sudo rm -rf /etc/docker sudo rm -rf /etc/apt/sources.list.d/docker.list sudo rm -rf /usr/share/keyrings/docker-archive-keyring.gpg sudo apt update docker --version Install Docker (Ubuntu 22.04) sudo apt update sudo apt install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg]","title":"Install Nvidia Docker on Ubuntu"},{"content":" Use local system volume mounts to start the runner container. docker run -d --name gitlab-runner --restart always \\ -v /srv/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest Create a group runner with a runner authentication token. üëâ Add group runner tag\nüëâ Get authentication token\nYou may refer to this article.\nRegister a runner Don\u0026rsquo;t forget to change the authentication token.\nRUNNER_TOKEN=\u0026#34;glrt-vTJPYBaajfypaShJH2xx\u0026#34; docker run --rm -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register \\ --non-interactive \\ --url \u0026#34;https://gitlab.com/\u0026#34; \\ --token \u0026#34;$RUNNER_TOKEN\u0026#34; \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image docker:stable-dind \\ --description \u0026#34;docker-runner\u0026#34; Edit /srv/gitlab-runner/config/config.toml Modify the following configurations:\nconcurrent: optional privileged volumes: docker in docker network_mode: optional /srv/gitlab-runner/config/config.toml concurrent = 4 # change check_interval = 0 shutdown_timeout = 0 [session_server] session_timeout = 1800 [[runners]] name = \u0026#34;docker-runner\u0026#34; url = \u0026#34;https://gitlab.com/\u0026#34; id = 28470179 token = \u0026#34;glrt-vTJPYBaajfypaShJH2Ge\u0026#34; token_obtained_at = 2023-10-14T12:45:43Z token_expires_at = 0001-01-01T00:00:00Z executor = \u0026#34;docker\u0026#34; [runners.cache] MaxUploadedArchiveSize = 0 [runners.docker] tls_verify = false image = \u0026#34;docker:stable-dind\u0026#34; privileged = true # change disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false #volumes = [\u0026#34;/cache\u0026#34;] # change volumes = [\u0026#34;/cache\u0026#34;, \u0026#34;/var/run/docker.sock:/run/docker.sock\u0026#34;] shm_size = 0 network_mode = \u0026#34;host\u0026#34; # add Restart Runner docker restart gitlab-runner References Install GitLab Runner Run GitLab Runner in a container Registering runners Create a group runner with a runner authentication token ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/install-gitlab-runner/","summary":"Setup Gitlab Runner in Container","title":"How to Setup Gitlab Runner in Container"},{"content":"Install Powerlevel10K üöÄ sudo apt update sudo apt install zsh # Install Oh My Zsh: sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install theme git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ~/.oh-my-zsh/custom/themes/powerlevel10k Install Zsh:\nIf you don\u0026rsquo;t have Zsh installed, install it using:\nsudo apt update sudo apt install zsh Install Oh My Zsh:\nInstall Oh My Zsh by running the following command:\nsh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Install Powerlevel10k Theme:\nClone the Powerlevel10k repository into the custom themes directory of Oh My Zsh:\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ~/.oh-my-zsh/custom/themes/powerlevel10k Set Zsh Theme:\nOpen your ~/.zshrc file in a text editor:\nnano ~/.zshrc Find the line that begins with ZSH_THEME and change it to:\nZSH_THEME=\u0026#34;powerlevel10k/powerlevel10k\u0026#34; Save the file and exit.\nInstall Required Fonts:\nPowerlevel10k uses special characters, so you need to install a font that includes these characters. Nerd Fonts are commonly used. Follow the instructions on the Nerd Fonts GitHub repository to install a compatible font.\nConfigure Powerlevel10k (Optional):\nWhen you start a new terminal session, Powerlevel10k will offer to run a configuration wizard. Follow the instructions to customize your prompt to your liking.\nRestart Zsh:\nRestart your terminal or run source ~/.zshrc to apply the changes.\nNow you should have Powerlevel10k installed and configured on your Ubuntu 22.04 system.\nInstall zsh-autosuggestions üöÄ Powerlevel10k does not change the default behavior of Zsh in terms of tab-completion; rather, it enhances and customizes the appearance of the prompt. The behavior of tab-completion is primarily controlled by the Zsh configuration.\nIf you want to enable suggestions during tab-completion in Zsh, you can use the zsh-autosuggestions plugin. This plugin provides fish-like autosuggestions as you type, and it works well with Powerlevel10k.\nHere are the steps to install and configure zsh-autosuggestions:\nInstall zsh-autosuggestions:\nYou can install it using your preferred method. One common way is via a plugin manager like oh-my-zsh.\nIf you are using oh-my-zsh, you can also manually clone the repository:\ngit clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions Enable zsh-autosuggestions:\nUpdate your ~/.zshrc file to enable the plugin. Add or modify the plugins line to include zsh-autosuggestions.\nplugins=(zsh-autosuggestions) Restart Zsh or Reload Configuration:\nEither restart your terminal or run the following command to apply the changes:\nsource ~/.zshrc Now, as you type, you should see autosuggestions appearing based on your command history.\n","permalink":"https://blog2.cmwang.net/en/posts/2023/10/install-powerlevel10k-on-ubuntu/","summary":"How to configure zsh and Powerlevel10k on Ubuntu","title":"Install Powerlevel10K on Ubuntu"},{"content":"Setting up a CNAME for your GitHub Pages does not require additional payment, but you need to have a private domain first. Here, we assume that we have already purchased a domain from Godaddy, so let\u0026rsquo;s go to the Godaddy DNS settings page to configure it.\nFirst, query GitHub\u0026rsquo;s IP address via Terminal: dig {Your Domain}.github.io dig xxxx.github.io Go to the Godaddy DNS page and set up 4 A Records and 1 CNAME Record, please refer to the image. Add a CNAME file with xxx.{Domain Name} to the private repo, ex. blog.xxx.com. Set up CNAME on the page and check the Enforce HTTPS option. It might take some time to take effect. If everything is set up correctly, the DNS Check in step 4 should pass.\nReferences Managing a custom domain for your GitHub Pages site ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/custom-domain-for-github-pages/","summary":"This article show you how to set up a custom domain for a GitHub Page, using Godaddy as an example.","title":"Configuring a custom domain for your GitHub Pages site"},{"content":"Welcome to the world of Hugo, where you can swiftly create your personal website! This guide will walk you through the process of creating a private website on GitHub and deploying it to GitHub Pages.\nWhat\u0026rsquo;s included in this guide .gitignore: Excludes files from version control. .github/workflows/hugo.yml: Uses GitHub Actions to deploy your private Hugo repository to a public GitHub Pages repository. Steps Create a private repository hugo-site on GitHub to manage your website source code, ensuring to include the README.md file.\nCreate a public repository {YOUR_USER_NAME}.github.io on GitHub to upload your static web pages to GitHub Pages.\nClone hugo-site to your local machine:\ngit clone https://github.com/{YOUR_USER_NAME}/hugo-site.git Alternatively, if you are using Git Submodules:\ngit clone --recursive https://github.com/{YOUR_USER_NAME}/hugo-site.git Create a Hugo project in the same directory as hugo-site (not inside hugo-site):\nhugo new site hugo-site --force Add an example theme. Here, we\u0026rsquo;ll use PaperMod as an example:\nAdd theme cd hugo-site git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod echo \u0026#34;theme : \u0026#39;PaperMod\u0026#39;\u0026#34; \u0026gt;\u0026gt; hugo.yaml Copy the .gitignore file into hugo-site.\n[Optional] Set up GitHub Action for automatic deployment:\nmkdir -p .github/workflows/ touch .github/workflows/hugo.yml Copy the contents of hugo.yml into the newly created hugo.yml file. Remember to modify these two parts:\ntoken: ${{ secrets.ACCESS_TOKEN }}: Use a personal access token for this private repository. repository-name: YOUR_USER_NAME}}/YOUR_USER_NAME.github.io: Replace YOUR_USER_NAME with your GitHub username. Test your Hugo website:\nhugo server -D Start writing your own posts:\nhugo new posts/20231006/index.md Begin writing in index.md inside content/posts/20231006, and place images in the same folder.\nCongratulations, you\u0026rsquo;ve configured your private repository! Don\u0026rsquo;t forget to commit your changes to the private repository. Best of luck creating your beautiful website!\nReferences How to setup ACCESS Token Using GitHub Actions to Publish Hugo Site From Private to Public Repo Learn to create a Hugo site in minutes. ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/hugo-get-started/","summary":"This guide will walk you through the process of creating a private website on GitHub and deploying it to GitHub Pages","title":"Hugo Deployment Guide"},{"content":"MacOS Installing via Homebrew Package Manager is the most convenient method.\nInstall Packages with Homebrew on macOS # install homebrew /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; # install hugo brew install hugo # verify hugo installation hugo version \u0026gt; hugo v0.119.0-b84644c008e0dc2c4b67bd69cccf87a41a03937e+extended darwin/amd64 BuildDate=2023-09-24T15:20:17Z VendorInfo=brew Windows Open powershell with Task Manager: Press win + x, navigate to Windows PowerShell (Admin), make sure to select Run as administrator.\nCheck the execution policy:\nGet-ExecutionPolicy If it shows Restricted, run the following command:\nSet-ExecutionPolicy AllSigned Install Chocolatey:\nSet-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) Verify the installation by typing in powershell:\nchoco # If you receive a response, the installation is successful Now, you can install Hugo using Chocolatey! üëáüëáüëá\nInstall Packages with Chocolatey on Windows # install hugo choco install hugo-extended # verify hugo installation hugo version \u0026gt; hugo v0.119.0-b84644c008e0dc2c4b67bd69cccf87a41a03937e... References Install Hugo on macOS Install Hugo on Windows ÊàëÁöÑ Windows Êñ∞Ë£ùÊ©ü Chocolatey ÂÆâË£ùÊ∏ÖÂñÆ ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/how-to-install-hugo/","summary":"We will introduce how to install hugo on your system in this post.","title":"How to Install Hugo"},{"content":"Ken Lin is my friend, and this is his AI clone-narrated version of Think Different.\n","permalink":"https://blog2.cmwang.net/en/posts/2023/10/think-different-ai-ken/","summary":"The narration generated by this AI is for entertainment purposes only and strictly prohibited for commercial use.","title":"Think Different - AI (Ken Lin)"},{"content":"After a few days of experimentation, I have officially relaunched my Blog on Github Pages. I often contemplate which platform to use for recording my thoughts, or what format to become familiar with, but nothing beats the authenticity of continuous documentation.\nHaving a notebook that I can control not only allows me to express my feelings but also enables me to record the bits and pieces of my learning journey. Hugo is truly an amazing tool; the overall experience is much better than it was with Hexo several years ago. In the upcoming posts, I will share some simple records on how to configure it on your own.\n","permalink":"https://blog2.cmwang.net/en/posts/2023/10/blog-reboot-2023/","summary":"In 2023, I\u0026rsquo;m going back to the simplicity of static webpages.","title":"Moving Back to Github Pages"},{"content":" Hello World!!\ngit clone https://github.com/{YOUR_USER_NAME}/hugo-site.git Let\u0026rsquo;s Do the Thing!\nimport sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Another Test\nSet some variables import sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Tip\nThis is a warning notice. Be warned!\n\u003c!DOCTYPE html\u003e ","permalink":"https://blog2.cmwang.net/en/posts/2023/10/hugo-test/","summary":"Hello World!!\ngit clone https://github.com/{YOUR_USER_NAME}/hugo-site.git Let\u0026rsquo;s Do the Thing!\nimport sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Another Test\nSet some variables import sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Tip\nThis is a warning notice.","title":"Hugo Test"},{"content":" green plums\nThe time leading up to and following the Tomb Sweeping Festival has become a significant rite for my family in recent years. We purchase green plums in advance from local farmers in Xinyi Township and use them to create preserved plums. However, due to insufficient rainfall this year, the fruit was slightly less than satisfactory compared to previous years.\nMaking preserved plums is a straightforward technique that doesn‚Äôt call for specialized equipment; rather, it just takes some time and perseverance. One kilogram of green plums, 100 grams of coarse salt, and 600 grams of sugar make up the recipe, which is quite straightforward. Please carry out the actions listed below:\nRub plums with salt\nWash the plums and remove the stems one by one. Blanch the plums by rubbing them with coarse salt for at least 15 minutes until the surface becomes moist and changes color. Tap the plums with a hammer one by one until they crack slightly, but do not smash them. Soak the plums in saltwater for 8 hours, making sure that the water covers them completely. Pour out the syrup and replace it with fresh syrup every 8 hours. Repeat step 5 for 3 days until the plums are fully infused with the sugar syrup. Drain the plums of any remaining bitter syrup, replace it with the final batch of sugar syrup, and let it sit for another 3 days to complete the process. I am grateful to my family for bringing a sense of ceremony into our daily lives. This ritual makes me appreciate the beauty of life.\nCrispy Plum\n","permalink":"https://blog2.cmwang.net/en/posts/2023/04/make-crispy-plum/","summary":"Making preserved plums is a straightforward technique that doesn‚Äôt call for specialized equipment","title":"Make Crispy Plum"},{"content":" Today you will have a good day. Jesus spoke to them at once, ‚ÄúDon‚Äôt be afraid,‚Äù he said. ‚ÄúTake courage! I am here!‚Äù (Mark 6:50)\n","permalink":"https://blog2.cmwang.net/en/posts/2023/04/you-will-have-a-good-day/","summary":"Today you will have a good day. Jesus spoke to them at once, ‚ÄúDon‚Äôt be afraid,‚Äù he said. ‚ÄúTake courage! I am here!‚Äù (Mark 6:50)","title":"You Will Have a Good Day"},{"content":" As I reflect on my five years in this company, I realize how much has happened and how much I have grown. From having a supervisor to being a lone ranger, to forming a small team and eventually founding the Nilvana, time has flown by quickly, and I am left wondering where tomorrow will take me.\nIt seems like just yesterday that I started my journey in this company. I remember the excitement of meeting new colleagues, learning new skills, and finding my place in the organization. Over time, I became more confident and began taking on more responsibilities. I was proud of my accomplishments and the impact that I was making.\nHowever, the road was not always easy. There were times when I faced challenges that made me question my abilities and my decisions. I learned that setbacks and failures are a part of the journey, and they can be a source of growth and resilience.\nOne of the most significant moments in my journey was the creation of the Nilvana brand. It was a leap of faith, but I knew that I had a vision that could change the game. With the help of a few dedicated team members, we worked tirelessly to bring the brand to life. Seeing it take off and gain recognition was one of the proudest moments of my career.\nLooking back on my journey, I realize that time truly flies. It seems like just yesterday that I started, but five years have gone by in a blink of an eye. As I prepare to take on new challenges, I am grateful for the experiences and the lessons that I have learned.\n","permalink":"https://blog2.cmwang.net/en/posts/2023/03/reflections-on-five-years-in-the-company/","summary":"As I reflect on my five years in this company, I realize how much has happened and how much I have grown","title":"Reflections on Five Years in the Company"},{"content":" This post explains how to increase the buffer memory for USB-FS devices on Linux systems in order to make full use of the imaging hardware‚Äôs capabilities. By default, USB-FS on Linux systems only allows 16 MB of buffer memory for all USB devices, which may not be sufficient for high-resolution cameras or multiple-camera set ups, resulting in image acquisition issues. To configure USB-FS and increase the buffer memory limit, the following steps should be taken:\nNote that GRUB is for desktop PC architecture. ARM embedded systems use a different bootloader, as GRUB requires a system with a BIOS, and embedded systems do not have one.\nCreate the file /etc/rc.local with the command sudo touch /etc/rc.local. This will create the file, allowing it to be edited. Change the permissions of the file with the command sudo chmod 744 /etc/rc.local. This will ensure that the file has the correct permissions to be edited. Change the buffer memory limit with the command echo 1000 \u0026amp;gt; /sys/module/usbcore/parameters/usbfs_memory_mb. This command will set the memory limit to 1000 MB, which should be enough to prevent image acquisition issues.` /etc/rc.local file contents example /etc/rc.local #!/bin/sh -e echo 1000 \u0026gt; /sys/module/usbcore/parameters/usbfs_memory_mb exit 0 After changing the memory limit, it is important to confirm the changes have been made correctly. This can be done by running the command:\ncat /sys/module/usbcore/parameters/usbfs_memory_mb which will display the current memory limit. If the limit is still 16 MB, then the changes will need to be made again. Additionally, further information about USB-FS on Linux can be found in the following sources:\nUnderstanding USBFS on Linux Increase USBFS Memory Limit in Ubuntu Change Kernel Cmdline by Edit /etc/default/grub Failed ","permalink":"https://blog2.cmwang.net/en/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/","summary":"This post explains how to increase the buffer memory for USB-FS devices on Linux systems.","title":"Configuring USB-FS for USB3 Vision Camera"},{"content":" Real-time object detection with CoreML is trickier than you think.\nUsually, you have two choices to build a machine learning app for your mobile device, inference can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.\nAt WWDC 2017 Apple released first Core ML. Core ML is Apple‚Äôs machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but that‚Äôs beyond the scope of this article. From the YOLOv7 official repository, we can get the export script to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. I‚Äôm also very glad to send a PR to improve the export script last night due to this writing üòÉ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. Matthijs Hollemans has already written an insightful article in his blog, be sure to checkout and support his efforts! Here is my short list:\nConfigure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model. Specify camera resolution, don‚Äôt simply select the highest resolution available if your app doesn‚Äôt require it. Resize or crop your input image to fit network input dimension, it depends on your application. Feed modified images to your model in a correct orientation. Fix Vision‚Äôs weird orin. Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation üòé. According to Hollemans‚Äôs article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.\nApple Developer Documentation | Recognizing Objects in Live Capture\nCreating a Custom Camera View | CodePath iOS Cliffnotes At the latest WWDC 2022, Apple introduced even more performance tools to its CoreML toolchain, now you can check your model‚Äôs metadata via performance reports and Core ML Instrument without writing any code. You can also use computeUnits = .cpuAndNeuralEngine if you don‚Äôt want to use the GPU but always force the model to run on the CPU and ANE if available.\nPrefer CPU and ANE instead of GPU.\nYou can learn more about ANE from the following repository, thank you again Hollemans.\nGitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)\nHere are snapshots from my model‚Äôs performance reports.\nYou can evaluate your model via drag-and-drop image files.\nThere is no significant inference speed differences among quantization models, but the model size only about half the size. It‚Äôs a good thing for your mobile applications. No inference speed improved. (Left is FP32, right is FP16)\nHalf the size of the FP32 model.\nFinally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatüî•. Happy coding!\nYolov7-tiny on iPad Mini 6. Copyrights of BBIBBI Dance Practice belongs Kakao Entertainment.\nReferences Recognizing Objects in Live Capture How to display Vision bounding boxes Creating a Custom Camera View The Neural Engine‚Ää‚Äî‚Ääwhat do we know about it? WongKinYiu/yolov7 WWDC2022‚Ää‚Äî‚ÄäOptimize your Core ML usage MobileNetV2 + SSDLite with Core ML yolov5‚Ää‚Äî‚ÄäCoreML Tools ","permalink":"https://blog2.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/","summary":"Real-time object detection with CoreML is trickier than you think.","title":"Execute YOLOv7 model on iOS devices"},{"content":" YOLOv7 TensorRT Performance Benchmarking.\nObject detection is one of the fundamental problems of computer vision. Instead of region detection and object classification separately in two stage detectors, object classification and bounding-box regression are done directly without using pre-generated region proposals in one stage detectors. YOLO (You Only Look Once) is one of the representative models of one-stage architecture. The YOLO family has continued to evolve since 2016, this summer we‚Äôve got its latest update to version 7.\nGitHub - WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new‚Ä¶\nIf you are trying to learn how to train your model on a custom dataset from the beginning, there are already many tutorials, notebooks and videos available online. In Nilvana, we really care about its real-world performance on the embedded devices, especially Nvidia Jetson family devices. So we conducted a series performance testing of YOLOv7 variants models on different devices, from cloud GPUs A100 to the latest tiny powerhouse AGX Orin.\nNVIDIA¬Æ Jetson AGX Orin‚Ñ¢ Developer Kit: smallest and most powerful AI edge computer\nThe main reason YOLOv7 is more accurate, compare to other models with similar AP, YOLOv7 has only about half computational cost.‚Ää‚Äî‚ÄäWongKinYiu \u0026gt; Input and Output shape of YOLOv7 (80 class)\nAccording to the results table, Xavier NX can run YOLOv7-tiny model pretty well. AGX Orin can even run YOLOv7x model more than 30 FPS, it‚Äôs amazing!\nEnd-to-End Performance on 1080P video, Batch Size=1\nPerformance Benchmarking Playlist\n","permalink":"https://blog2.cmwang.net/en/posts/2022/07/performance-benchmarking-of-yolov7-tensorrt/","summary":"I will conduct a series performance test of YOLOv7 variants models from cloud GPUs to the latest powerhouse AGX Orin in this post.","title":"Performance Benchmarking of YOLOv7 TensorRT"},{"content":"Expanding Our Horizons Overview Object Encapsulation Inheritance Handling variation Commonality and variability analysis Abstract class and its derived classes Objects: Traditional view and new view Traditional View Data with methods - smart data too narrow from implementation perspective Broad View From conceptual perspective an object is an entity that has responsibilities (Ë≤¨‰ªª), these responsibilities define the behavior of the object. Or an entity that has specific behavior (ÁâπÂÆöË°åÁÇ∫). Focus on intention/motivation not implementation This view enables us to build software in two steps:\nMake a preliminary design without worrying about all the details involved. Implement the design. The reason this works is that we only have to focus on the object‚Äôs public interface ‚Äî the communication window through which I ask the object to do something.\nHiding implementations behind interfaces essentially decouples them from the using objects.\nEncapsulation: Traditional view and new view Traditional View data hiding Broad View any kind of hiding Implementations (data, methods..) Drived classes (Encapsulation of type is achieved when there is an abstract class with derivations (or an interface with implementations) that are used polymorphically) Design details Instantiation rules (ex. creational patterns) Advantage It gives us a better way to split up (decompose) our programs. The encapsulating layers become the interfaces we design to. (Â∞ÅË£ùÂ±§ÊàêÁÇ∫Ë®≠Ë®àÈúÄË¶ÅÈÅµÂæ™ÁöÑ‰ªãÈù¢)\nBy encapsulating different kinds of subclasses (encapsulation of type), we can add new ones without changing any of the client programs using them. (GoF typically means when they mention encapsulation)\nInheritance Traditional View reuse of classes achived by creating classes and then deriving new (spcialized) classes bases on these base (generalized) classes Broad View using inheritance for specialization, however can cause weak cohesion reduces possibility of reuse does not scale well with variation to classify classes as things that behave the same way. (placeholder) Find What Is Varying and Encapsulate It Consider what should be variable in your design. This approach is the opposite of focusing on the cause of redesign. Instead of considering what might force a change to a design, consider what you want to be able to change without redesign. The focus here is on encapsulating the concept that varies, a theme of many design patterns. \u0026ndash; GoF, Design Patterns\nMore about GoF\u0026rsquo;s Encapsulation Design Patterns use inheritance to classify variations in behaviors. Hiding classes with an abstract class or interface ‚Äî type encapsulation. Containing a reference of this abstract class or interface type (aggregation) hides these derived classes that represent variations in behavior. In effect, many design patterns use encapsulation to create layers between objects. Containing variation in data vs containing variation in behavior Handling variation in data Have a data member that tells me what type of movement my object has. Have two different types of Animals (both derived from the base Animal class) ‚Äî one for walking and one for flying. Handling variation in behavior with objects Using objects to contain variation in attributes and using objects to contain variation in behavior are very similar. Don\u0026rsquo;t afraid.\nCommonality and Variability Identify where things vary (commonality analysis) and then identify how they vary (variability analysis).\nCommonality analysis is the search for common elements that helps us understand how family members are the same.\nVariability analysis reveals how family members vary. Variability only makes sense within a given commonality.\nEx. Whiteboard marker, pencil, ballpoint pen\nCommonality: writing instrument Variability: material to write, shape.. Commonality and Variability and Abstract class Commonality analysis seeks structure that is unlikely to change over time, while variability analysis captures structure that is likely to change. Variability analysis makes sense only in terms of the context defined by the associated commonality analysis. In other words, if variations are the specific concrete cases in the domain, commonality defines the concepts in the domain that tie them together. The common concepts will be represented by abstract classes. The variations found by variability analysis will be implemented by the concrete classes.\nRelationship between Commonality and Variability, perspectives, and abstract classes Benefits of using abstract classes for specialization Two-Step Procedure for Design Ask yourself:\nWhen defining an abstract class (commonality): What interface is needed to handle all the responsibilities (core concepts from the conceptual perspective) of this class? When defining derived classes: Given this particular implementation (this variation), how can I implement it (variation) with the given specification? Take away Think object-oriented in a broad way.\nObject: an entity that has responsibilities (specific behavior) Encapsulation: any kind of hiding (instantiation rule, type..) Inheritance: use for specialization and classify classes as things that behave the same way. Find what is varying and encapsulate it (in behavior).\nCommonality, variability and abstract class: use inheritance to classify variations in behaviors.\n","permalink":"https://blog2.cmwang.net/en/posts/2017/01/design-patterns-explained-ch8/","summary":"\u003ch1 id=\"expanding-our-horizons\"\u003eExpanding Our Horizons\u003c/h1\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eObject\u003c/li\u003e\n\u003cli\u003eEncapsulation\u003c/li\u003e\n\u003cli\u003eInheritance\u003c/li\u003e\n\u003cli\u003eHandling variation\u003c/li\u003e\n\u003cli\u003eCommonality and variability analysis\u003c/li\u003e\n\u003cli\u003eAbstract class and its derived classes\u003c/li\u003e\n\u003c/ul\u003e","title":"Design Patterns Explained - CH8"}]