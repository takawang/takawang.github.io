<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>iOS on Liberation Notes</title>
    <link>https://blog2.cmwang.net/en/tags/ios/</link>
    <description>Recent content in iOS on Liberation Notes</description>
    <image>
      <title>Liberation Notes</title>
      <url>https://blog2.cmwang.net/47</url>
      <link>https://blog2.cmwang.net/47</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 06 Aug 2022 11:04:15 +0000</lastBuildDate>
    <atom:link href="https://blog2.cmwang.net/en/tags/ios/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Execute YOLOv7 model on iOS devices</title>
      <link>https://blog2.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/</link>
      <pubDate>Sat, 06 Aug 2022 11:04:15 +0000</pubDate>
      <guid>https://blog2.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/</guid>
      <description>Real-time object detection with CoreML is trickier than you think.</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.jpeg" alt="image"  />

Real-time object detection with CoreML is trickier than you think.</p>
<p>Usually, you have two choices to build a machine learning app for your mobile device, <strong>inference</strong> can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.</p>
<p>At WWDC 2017 Apple released first Core ML. Core ML is Appleâ€™s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but thatâ€™s beyond the scope of this article. From the <a href="https://github.com/WongKinYiu/yolov7">YOLOv7 official repository</a>, we can get the <a href="https://github.com/WongKinYiu/yolov7/blob/main/export.py">export script</a> to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. Iâ€™m also very glad to send a <a href="https://github.com/WongKinYiu/yolov7/pull/434/commits">PR</a> to improve the export script last night due to this writing ðŸ˜ƒ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. <a href="https://twitter.com/mhollemans">Matthijs Hollemans</a> has already written an insightful <a href="https://machinethink.net/blog/bounding-boxes/">article</a> in his blog, be sure to checkout and <a href="https://leanpub.com/coreml-survival-guide">support his efforts</a>! Here is my short list:</p>
<ul>
<li>Configure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model.</li>
<li>Specify camera resolution, donâ€™t simply select the highest resolution available if your app doesnâ€™t require it.</li>
<li>Resize or crop your input image to fit network input dimension, it depends on your application.</li>
<li>Feed modified images to your model in a correct orientation.</li>
<li>Fix Visionâ€™s weird orin.</li>
<li>Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation ðŸ˜Ž.</li>
</ul>
<p>According to Hollemansâ€™s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.</p>
<p><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Apple Developer Documentation | Recognizing Objects in Live Capture</a></p>
<p><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View | CodePath iOS Cliffnotes</a>
At the latest <a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC 2022</a>, Apple introduced even more performance tools to its CoreML toolchain, now you can check your modelâ€™s metadata via <strong>performance reports</strong> and <strong>Core ML Instrument</strong> without writing any code. You can also use <code>computeUnits = .cpuAndNeuralEngine</code> if you donâ€™t want to use the GPU but always force the model to run on the CPU and ANE if available.</p>
<p><img loading="lazy" src="images/2.png#layoutTextWidth" alt="image"  />

Prefer CPU and ANE instead of GPU.</p>
<p>You can learn more about ANE from the following repository, thank you again Hollemans.</p>
<p><a href="https://github.com/hollance/neural-engine">GitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)</a></p>
<p>Here are snapshots from my modelâ€™s performance reports.</p>
<p><img loading="lazy" src="images/3.png#layoutTextWidth" alt="image"  />

You can evaluate your model via drag-and-drop image files.</p>
<p>There is no significant inference speed differences among quantization models, but the model size only about half the size. Itâ€™s a good thing for your mobile applications.
<img loading="lazy" src="images/4.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/5.png#layoutTextWidth" alt="image"  />

No inference speed improved. (Left is FP32, right is FP16)</p>
<p><img loading="lazy" src="images/6.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/7.png#layoutTextWidth" alt="image"  />

Half the size of the FP32 model.</p>
<p>Finally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatðŸ”¥. Happy coding!</p>
<p><img loading="lazy" src="images/8.gif#layoutTextWidth" alt="image"  />

<strong>Yolov7-tiny on iPad Mini 6.</strong> Copyrights of <a href="https://youtu.be/hngml3y2Rq8">BBIBBI Dance Practice</a> belongs Kakao Entertainment.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Recognizing Objects in Live Capture</a></li>
<li><a href="https://machinethink.net/blog/bounding-boxes/">How to display Vision bounding boxes</a></li>
<li><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View</a></li>
<li><a href="https://github.com/hollance/neural-engine">The Neural Engineâ€Šâ€”â€Šwhat do we know about it?</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7">WongKinYiu/yolov7</a></li>
<li><a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC2022â€Šâ€”â€ŠOptimize your Core ML usage</a></li>
<li><a href="https://machinethink.net/blog/mobilenet-ssdlite-coreml/">MobileNetV2 + SSDLite with Core ML</a></li>
<li><a href="https://github.com/dbsystel/yolov5-coreml-tools">yolov5â€Šâ€”â€ŠCoreML Tools</a></li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
